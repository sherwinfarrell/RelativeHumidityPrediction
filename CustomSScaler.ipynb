{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the Data and The Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sherwin\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3071: DtypeWarning: Columns (1,2,4,5,6,7,8,9,10,11,12,13,14,15) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"HumidityDataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping all the libraires and selecting only rows that start after the 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"longitude\",\"latitude\", \"WaveHeight\", \"WavePeriod\", \"MeanWaveDirection\", \"Hmax\",\"QC_Flag\"],inplace = True, axis = 1)\n",
    "df = df.iloc[331371:]\n",
    "buoy_ident = { 'M2':1 , 'M3': 2, 'M4':3, 'M5': 4, 'M6': 5}\n",
    "# df.station_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting only the values of rows that have buoys ident as M2 to M6\n",
    "df = df.loc[df.station_id.isin(buoy_ident.keys()) ]\n",
    "df = df.drop([\"time\"], axis = 1)\n",
    "# df.station_id.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({ 'station_id': buoy_ident})\n",
    "df = df.dropna(axis = 1, how='all')\n",
    "df.reset_index(inplace = True)\n",
    "df.dropna(inplace = True, how = 'all')\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.drop('index', axis = 1, inplace = True)\n",
    "# df.reset_index(inplace= True)\n",
    "# df.isna()\n",
    "# drop('index',axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0 1.0 0.044256490952006296 ... 0.0054091266719118805\n",
      "  0.000983477576711251 0.006195908733280882]\n",
      " [0.0 1.0 0.06483790523690773 ... 0.005286783042394016\n",
      "  0.0031920199501246876 0.005187032418952618]\n",
      " [0.0 1.0 0.25310222572385266 ... 0.011621036044908412\n",
      "  0.011128619263344497 0.01132558597597006]\n",
      " ...\n",
      " [5 1006.9 340.0 ... 14.0 10.6 15.0]\n",
      " [4 1001.4 160.0 ... 9.3 8.6 9.4]\n",
      " [3 1011.0 200.0 ... 14.6 11.8 15.1]]\n"
     ]
    }
   ],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(\n",
    "    df.iloc[:,:-1].values,df.iloc[:,-1].values, test_size=0.2)\n",
    "\n",
    "\n",
    "class CustomScaler():\n",
    "    \n",
    "        def __init__(self, X_train, y_train ) :\n",
    "        \n",
    "            self.X_train =   X_train \n",
    "            self.y_train =   y_train\n",
    "            #self.x_means =   [np.mean(self.X_train[i]) for i in range(self.X_train.shape[1]) ]\n",
    "            self.x_means =   [np.min(self.X_train[i]) for i in range(self.X_train.shape[1])]\n",
    "\n",
    "            self.x_maxs =    [np.max(self.X_train[i]) for i in range(self.X_train.shape[1])]\n",
    "            self.x_mins =    [np.min(self.X_train[i]) for i in range(self.X_train.shape[1])]\n",
    "#             self.y_mean =    np.mean(self.y_train)\n",
    "            self.y_mean =    np.min(self.y_train)\n",
    "            self.y_max_min = np.max(self.y_train) - np.min(self.y_train)\n",
    "\n",
    "        def scaleX(self, x_value):\n",
    "            x = x_value.copy()\n",
    "            for i in range(x.shape[1]):\n",
    "                x[i]= (x[i] - self.x_means[i])/(self.x_maxs[i]-self.x_mins[i])\n",
    "            return x\n",
    "\n",
    "        def inverseScaleX(self, x_value):\n",
    "            x = x_value.copy()\n",
    "            \n",
    "            for i in range(x.shape[1]):\n",
    "                x[i]= (x[i] * (self.x_maxs[i]-self.x_mins[i])) + self.x_means[i]\n",
    "            return x\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        def scaleY(self, y_value):\n",
    "            y = y_value.copy()\n",
    "            ys = (y - self.y_mean)/(self.y_max_min)\n",
    "            return ys\n",
    "\n",
    "        def inverseScaleY(self, y_value):\n",
    "            y = y_value.copy()\n",
    "            ys = (y * self.y_max_min) + self.y_mean\n",
    "            return ys\n",
    "\n",
    "\n",
    "\n",
    "custom_scaler = CustomScaler(X_train1.copy(),y_train1.copy())\n",
    "    \n",
    "X_train = custom_scaler.scaleX(X_train1.copy())\n",
    "y_train = custom_scaler.scaleY(y_train1.copy())\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train1 - np.min(X_train1))/(np.max(X_train1) - np.min(X_train1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model1():    \n",
    "    model = keras.Sequential()\n",
    "    model.add(Dense(10, activation = \"relu\",input_dim = 8, name = \"layer1\"))\n",
    "    model.add(Dense(20, activation = \"relu\", name = \"layer2\"))\n",
    "    model.add(Dense(20, activation = \"relu\", name = \"layer5\"))\n",
    "    model.add(Dense(20, activation = \"relu\", name = \"layer6\"))\n",
    "\n",
    "    model.add(Dense(1, kernel_initializer='normal', name = \"layer7\"))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer='SGD')\n",
    "    return model\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
    "\n",
    "estimator = KerasRegressor(build_fn=model1, epochs=1000, batch_size=4096, verbose=1,callbacks=[es])\n",
    "history=estimator.fit(np.asarray(X_train).astype('float32'),np.asarray(y_train).astype('float32'))\n",
    "\n",
    "\n",
    "\n",
    "#history = model.fit(x_pca, df.iloc[:, -1], batch_size=batch_size, epochs=20, validation_split=0.1)\n",
    "# estimator = KerasRegressor(build_fn=model, epochs=100, batch_size=batch_size, verbose=0)\n",
    "# history = model.fit(np.asarray(x).astype('float32'),np.asarray(y).astype('float32') , epochs=150, batch_size=50,  verbose=1, validation_split=0.3)\n",
    "# kfold = KFold(n_splits=5)\n",
    "# results = cross_val_score(estimator,np.asarray(x).astype('float32'), np.asarray(y).astype('float32'), cv=kfold)\n",
    "# print(\"Results: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "y_pred = estimator.predict(X_test)\n",
    "# y_pred = max_min*y_pred + y_min\n",
    "print(mean_squared_error(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.column_stack((y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predTest = scaler.inverse_transform(np.column_stack((X_test, y_pred)))\n",
    "ogTest = scaler.inverse_transform(np.column_stack((X_test, y_test)))\n",
    "np.column_stack((predTest[:, -1], ogTest[:, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
